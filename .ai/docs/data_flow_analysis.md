# Data Flow Analysis

## Data Models Overview

The application uses a consistent set of data models across the Python (FastAPI) backend and the TypeScript (Next.js) frontend to represent repositories, wiki structures, and chat interactions.

### Core Backend Models (Pydantic)
- **`WikiPage`**: Represents a single documentation page.
    - `id`, `title`, `content`, `filePaths` (List[str]), `importance` (high/medium/low), `relatedPages` (List[str]).
- **`WikiStructureModel`**: Defines the hierarchy of the generated wiki.
    - `id`, `title`, `description`, `pages` (List[WikiPage]), `sections` (List[WikiSection]), `rootSections` (List[str]).
- **`RepoInfo`**: Captures repository metadata.
    - `owner`, `repo`, `type` (github/gitlab/bitbucket/local), `token` (optional), `localPath`, `repoUrl`.
- **`ChatCompletionRequest`**: Standardizes the payload for chat interactions.
    - `repo_url`, `messages` (List[ChatMessage]), `filePath` (optional), `token`, `provider`, `model`, `language`, `filters` (included/excluded dirs/files).
- **`WikiCacheData`**: The structure used for persisting generated wikis.
    - `wiki_structure`, `generated_pages` (Dict[str, WikiPage]), `repo`, `provider`, `model`.

### Core Frontend Models (TypeScript)
- **`WikiPage`**, **`WikiStructure`**, **`RepoInfo`**: Mirror the backend Pydantic models for type safety.
- **`Message`**: Represents a chat message with `role` (user/assistant/system) and `content`.
- **`ResearchStage`**: Used in "Deep Research" mode to track the progress of multi-turn investigations.

## Data Transformation Map

Data undergoes several transformations as it moves from raw repository files to structured documentation and AI-generated responses.

1.  **Repository Ingestion**:
    - Raw files are fetched via `git clone` or API calls (GitHub/GitLab/Bitbucket).
    - `read_all_documents` transforms files into `Document` objects, attaching metadata like `file_path`, `token_count`, and `is_code`.
2.  **RAG Pipeline Transformation**:
    - `TextSplitter` breaks documents into manageable chunks based on token limits.
    - `ToEmbeddings` (via `adalflow`) transforms text chunks into vector representations using the configured embedder (OpenAI, Google, or Ollama).
    - `LocalDB` persists these transformed documents and their vectors into a FAISS index.
3.  **Retrieval Transformation**:
    - User queries are embedded using the same embedding model.
    - `FAISSRetriever` performs a similarity search to find the most relevant document chunks.
4.  **Generation Transformation**:
    - A prompt is constructed by combining a `system_prompt`, `conversation_history`, `retrieved_context`, and the `user_query`.
    - The LLM transforms this prompt into a structured `RAGAnswer` or a streamed text response.
5.  **Wiki Generation**:
    - The repository structure is analyzed by the LLM to generate a `WikiStructureModel`.
    - Each page in the structure is then individually generated by the LLM using relevant file contexts.

## Storage Interactions

The system utilizes both server-side and client-side storage for persistence and performance.

### Server-Side Storage (`~/.adalflow/`)
-   **`/repos/`**: Stores cloned Git repositories.
-   **`/databases/`**: Stores serialized `LocalDB` states (FAISS indices and document metadata) as `.pkl` files.
-   **`/wikicache/`**: Stores generated wiki data as JSON files, named using the pattern `deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json`.

### Client-Side Storage
-   **`localStorage`**: The frontend caches generated wiki structures and pages to provide near-instant loading for previously visited repositories.
-   **Session State**: React state manages the current conversation history and active wiki page during a user session.

## Validation Mechanisms

Data integrity and system stability are maintained through several validation layers:

-   **Pydantic Validation**: All API endpoints use Pydantic models to validate incoming request bodies and outgoing responses.
-   **Token Limit Validation**: `tiktoken` is used to estimate token counts for user queries and document chunks, ensuring they stay within LLM and embedding model limits (e.g., `MAX_INPUT_TOKENS = 7500`).
-   **Embedding Consistency**: The `RAG` component includes a `_validate_and_filter_embeddings` method that ensures all vectors in the FAISS index have consistent dimensions, filtering out any documents that failed to embed correctly.
-   **Authentication**: Sensitive operations like deleting a wiki cache are protected by `WIKI_AUTH_MODE` and `WIKI_AUTH_CODE` environment variables.
-   **File Filtering**: Inclusion and exclusion rules (e.g., `DEFAULT_EXCLUDED_DIRS`) prevent sensitive or irrelevant files (like `.git`, `node_modules`, or binary files) from entering the data pipeline.

## State Management Analysis

### Backend State
-   **`Memory` Component**: Manages conversation state within the `RAG` pipeline using a list of `DialogTurn` objects.
-   **`DatabaseManager`**: Maintains the state of the currently loaded vector database and repository paths.

### Frontend State
-   **`Ask` Component**: Manages `conversationHistory` (array of `Message`) and `researchStages` for the Deep Research feature.
-   **`RepoWikiPage`**: Manages the `wikiStructure`, `generatedPages` (a map of page IDs to content), and `pagesInProgress` (a set of IDs currently being generated to prevent duplicate requests).
-   **`LanguageContext`**: Manages the application's internationalization state.

## Serialization Processes

-   **JSON Serialization**: Used for all REST API communication, WebSocket messages, and the persistence of wiki cache files.
-   **Pickle Serialization**: Used by `adalflow`'s `LocalDB` to save and load the state of the FAISS index and document objects.
-   **Base64 Encoding**: Used when fetching file contents via the GitHub API.
-   **URL Encoding**: Used for safely passing repository URLs and file paths in API requests.

## Data Lifecycle Diagrams

### Chat Interaction Lifecycle
1.  **Input**: User sends a message via the `Ask` component.
2.  **Transport**: Message is wrapped in a `ChatCompletionRequest` and sent over a **WebSocket** (`/ws/chat`).
3.  **Processing**: Backend prepares the `RAG` instance, retrieves context from the FAISS index, and constructs the LLM prompt.
4.  **Generation**: LLM generates a response, which is streamed back to the client.
5.  **Update**: Frontend appends the response to `conversationHistory` and renders it via the `Markdown` component.

### Wiki Generation Lifecycle
1.  **Trigger**: User navigates to a repository page.
2.  **Cache Check**: Frontend checks `localStorage`, then queries `/api/wiki_cache`.
3.  **Structure Generation**: If not cached, backend analyzes the repo and returns a `WikiStructureModel`.
4.  **Page Generation**: Frontend iterates through the structure, requesting content for each `WikiPage` via the backend.
5.  **Persistence**: As pages are generated, they are saved to the server-side cache and the client's `localStorage`.
6.  **Display**: `WikiTreeView` renders the hierarchy, and selecting a page displays its content in the main view.