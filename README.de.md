# DeepWiki-Open

![DeepWiki Banner](screenshots/Deepwiki.png)

**DeepWiki** ist mein eigener Implementierungsversuch von DeepWiki, das automatisch wundersch√∂ne, interaktive Wikis f√ºr jedes GitHub-, GitLab- oder Bitbucket-Repository erstellt! Geben Sie einfach einen Repository-Namen ein, und DeepWiki wird:

1. Die Code-Struktur analysieren
2. Umfassende Dokumentation generieren
3. Visuelle Diagramme erstellen, um die Funktionsweise zu erkl√§ren
4. Alles in einem leicht zu navigierenden Wiki organisieren

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)
[![Tip in Crypto](https://tip.md/badge.svg)](https://tip.md/sng-asyncfunc)
[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](./README.zh.md) | [ÁπÅÈ´î‰∏≠Êñá](./README.zh-tw.md) | [Êó•Êú¨Ë™û](./README.ja.md) | [Espa√±ol](./README.es.md) | [ÌïúÍµ≠Ïñ¥](./README.kr.md) | [Ti·∫øng Vi·ªát](./README.vi.md) | [Portugu√™s Brasileiro](./README.pt-br.md) | [Fran√ßais](./README.fr.md) | [–†—É—Å—Å–∫–∏–π](./README.ru.md) | [Deutsch](./README.de.md)

## ‚ú® Features

- **Instant-Dokumentation**: Verwandeln Sie jedes GitHub-, GitLab- oder Bitbucket-Repository in Sekunden in ein Wiki
- **Unterst√ºtzung f√ºr private Repositories**: Sicherer Zugriff auf private Repositories mit pers√∂nlichen Zugriffstokens
- **Intelligente Analyse**: KI-gest√ºtztes Verst√§ndnis von Code-Struktur und Beziehungen
- **Wundersch√∂ne Diagramme**: Automatische Mermaid-Diagramme zur Visualisierung von Architektur und Datenfluss
- **Einfache Navigation**: Einfache, intuitive Benutzeroberfl√§che zum Erkunden des Wikis
- **Frage-Feature**: Chatten Sie mit Ihrem Repository mithilfe von KI-gest√ºtztem RAG f√ºr genaue Antworten
- **DeepResearch**: Mehrteiliger Rechercheprozess, der komplexe Themen gr√ºndlich untersucht
- **Mehrere Model-Provider**: Unterst√ºtzung f√ºr Google Gemini, OpenAI, OpenRouter und lokale Ollama-Modelle
- **Flexible Embeddings**: W√§hlen Sie zwischen OpenAI, Google AI oder lokalen Ollama-Embeddings f√ºr optimale Leistung

## üöÄ Schnellstart (Sehr einfach!)

### Option 1: Mit Docker

```bash
# Repository klonen
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Erstellen Sie eine .env-Datei mit Ihren API-Schl√ºsseln
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
# Optional: Verwenden Sie Google AI Embeddings statt OpenAI (empfohlen bei Google-Modellen)
echo "DEEPWIKI_EMBEDDER_TYPE=google" >> .env
# Optional: F√ºgen Sie OpenRouter API-Schl√ºssel hinzu, wenn Sie OpenRouter-Modelle verwenden m√∂chten
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env
# Optional: F√ºgen Sie Ollama-Host hinzu, wenn nicht lokal. Standard: http://localhost:11434
echo "OLLAMA_HOST=your_ollama_host" >> .env
# Optional: F√ºgen Sie Azure API-Schl√ºssel, Endpoint und Version hinzu, wenn Sie Azure OpenAI-Modelle verwenden m√∂chten
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" >> .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" >> .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version" >> .env
# Starten Sie mit Docker Compose
docker-compose up
```

Detaillierte Anweisungen zur Verwendung von DeepWiki mit Ollama und Docker finden Sie unter [Ollama-Anweisungen](Ollama-instruction.md).

> üí° **Wo erhalten Sie diese Schl√ºssel:**
> - Holen Sie sich einen Google API-Schl√ºssel von [Google AI Studio](https://makersuite.google.com/app/apikey)
> - Holen Sie sich einen OpenAI API-Schl√ºssel von [OpenAI Platform](https://platform.openai.com/api-keys)
> - Holen Sie sich Azure OpenAI-Anmeldedaten von [Azure Portal](https://portal.azure.com/) - erstellen Sie eine Azure OpenAI-Ressource und rufen Sie den API-Schl√ºssel, Endpoint und API-Version ab

### Option 2: Manuelle Einrichtung (Empfohlen)

#### Schritt 1: Richten Sie Ihre API-Schl√ºssel ein

Erstellen Sie eine `.env`-Datei im Projekt-Stammverzeichnis mit diesen Schl√ºsseln:

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Verwenden Sie Google AI Embeddings (empfohlen bei Google-Modellen)
DEEPWIKI_EMBEDDER_TYPE=google
# Optional: F√ºgen Sie dies hinzu, wenn Sie OpenRouter-Modelle verwenden m√∂chten
OPENROUTER_API_KEY=your_openrouter_api_key
# Optional: F√ºgen Sie dies hinzu, wenn Sie Azure OpenAI-Modelle verwenden m√∂chten
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_VERSION=your_azure_openai_version
# Optional: F√ºgen Sie Ollama-Host hinzu, wenn nicht lokal. Standard: http://localhost:11434
OLLAMA_HOST=your_ollama_host
```

#### Schritt 2: Starten Sie das Backend

```bash
# Installieren Sie Python-Abh√§ngigkeiten
python -m pip install poetry==1.8.2 && poetry install -C api

# Starten Sie den API-Server
python -m api.main
```

#### Schritt 3: Starten Sie das Frontend

```bash
# Installieren Sie JavaScript-Abh√§ngigkeiten
npm install
# oder
yarn install

# Starten Sie die Web-App
npm run dev
# oder
yarn dev
```

#### Schritt 4: Verwenden Sie DeepWiki!

1. √ñffnen Sie [http://localhost:3000](http://localhost:3000) in Ihrem Browser
2. Geben Sie ein GitHub-, GitLab- oder Bitbucket-Repository ein (wie `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab` oder `https://bitbucket.org/redradish/atlassian_app_versions`)
3. F√ºr private Repositories klicken Sie auf "+ Zugriffstokens hinzuf√ºgen" und geben Sie Ihren pers√∂nlichen GitHub- oder GitLab-Zugrifftoken ein
4. Klicken Sie auf "Wiki generieren" und sehen Sie die Magie geschehen!

## üîç Wie es funktioniert

DeepWiki verwendet KI zum:

1. Klonen und Analysieren des GitHub-, GitLab- oder Bitbucket-Repositorys (einschlie√ülich privater Repos mit Token-Authentifizierung)
2. Erstellen von Code-Embeddings f√ºr intelligente Abfrage
3. Generieren von Dokumentation mit kontextbewusster KI (mit Google Gemini, OpenAI, OpenRouter, Azure OpenAI oder lokalen Ollama-Modellen)
4. Erstellen von visuellen Diagrammen zur Erkl√§rung von Code-Beziehungen
5. Organisieren alles in ein strukturiertes Wiki
6. Erm√∂glichen intelligenter Fragen und Antworten mit dem Repository √ºber die Ask-Funktion
7. Bereitstellung von umfassenden Forschungsf√§higkeiten mit DeepResearch

```mermaid
graph TD
    A[Benutzer gibt GitHub/GitLab/Bitbucket Repo ein] --> AA{Privates Repo?}
    AA -->|Ja| AB[Zugrifftoken hinzuf√ºgen]
    AA -->|Nein| B[Repository klonen]
    AB --> B
    B --> C[Code-Struktur analysieren]
    C --> D[Code-Embeddings erstellen]

    D --> M{Model Provider ausw√§hlen}
    M -->|Google Gemini| E1[Mit Gemini generieren]
    M -->|OpenAI| E2[Mit OpenAI generieren]
    M -->|OpenRouter| E3[Mit OpenRouter generieren]
    M -->|Local Ollama| E4[Mit Ollama generieren]
    M -->|Azure| E5[Mit Azure generieren]

    E1 --> E[Dokumentation generieren]
    E2 --> E
    E3 --> E
    E4 --> E
    E5 --> E

    D --> F[Visuelle Diagramme erstellen]
    E --> G[Als Wiki organisieren]
    F --> G
    G --> H[Interaktives DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
```

## üõ†Ô∏è Projektstruktur

```
deepwiki/
‚îú‚îÄ‚îÄ api/                  # Backend API-Server
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # API-Einstiegspunkt
‚îÇ   ‚îú‚îÄ‚îÄ api.py            # FastAPI-Implementierung
‚îÇ   ‚îú‚îÄ‚îÄ rag.py            # Retrieval Augmented Generation
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline.py  # Datenverarbeitungsprogramme
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt  # Python-Abh√§ngigkeiten
‚îÇ
‚îú‚îÄ‚îÄ src/                  # Frontend Next.js App
‚îÇ   ‚îú‚îÄ‚îÄ app/              # Next.js App-Verzeichnis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx      # Hauptanwendungsseite
‚îÇ   ‚îî‚îÄ‚îÄ components/       # React-Komponenten
‚îÇ       ‚îî‚îÄ‚îÄ Mermaid.tsx   # Mermaid-Diagramm-Renderer
‚îÇ
‚îú‚îÄ‚îÄ public/               # Statische Ressourcen
‚îú‚îÄ‚îÄ package.json          # JavaScript-Abh√§ngigkeiten
‚îî‚îÄ‚îÄ .env                  # Umgebungsvariablen (erstellen Sie diese)
```

## ü§ñ Provider-basiertes Modellauswahlsystem

DeepWiki implementiert nun ein flexibles, provider-basiertes Modellauswahlsystem, das mehrere LLM-Provider unterst√ºtzt:

### Unterst√ºtzte Provider und Modelle

- **Google**: Standard `gemini-2.5-flash`, unterst√ºtzt auch `gemini-2.5-flash-lite`, `gemini-2.5-pro`, etc.
- **OpenAI**: Standard `gpt-5-nano`, unterst√ºtzt auch `gpt-5`, `4o`, etc.
- **OpenRouter**: Zugriff auf mehrere Modelle √ºber eine einheitliche API, einschlie√ülich Claude, Llama, Mistral, etc.
- **Azure OpenAI**: Standard `gpt-4o`, unterst√ºtzt auch `o4-mini`, etc.
- **Ollama**: Unterst√ºtzung f√ºr lokal laufende Open-Source-Modelle wie `llama3`

### Umgebungsvariablen

Jeder Provider ben√∂tigt seine entsprechenden API-Schl√ºssel-Umgebungsvariablen:

```
# API-Schl√ºssel
GOOGLE_API_KEY=your_google_api_key        # Erforderlich f√ºr Google Gemini-Modelle
OPENAI_API_KEY=your_openai_api_key        # Erforderlich f√ºr OpenAI-Modelle
OPENROUTER_API_KEY=your_openrouter_api_key # Erforderlich f√ºr OpenRouter-Modelle
AZURE_OPENAI_API_KEY=your_azure_openai_api_key  # Erforderlich f√ºr Azure OpenAI-Modelle
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  # Erforderlich f√ºr Azure OpenAI-Modelle
AZURE_OPENAI_VERSION=your_azure_openai_version  # Erforderlich f√ºr Azure OpenAI-Modelle

# OpenAI API Base URL Konfiguration
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, f√ºr benutzerdefinierte OpenAI API Endpoints

# Ollama Host
OLLAMA_HOST=your_ollama_host # Optional, wenn Ollama nicht lokal ist. Standard: http://localhost:11434

# Konfigurationsverzeichnis
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, f√ºr benutzerdefinierten Konfigurationsdatei-Speicherort
```

### Konfigurationsdateien

DeepWiki verwendet JSON-Konfigurationsdateien zur Verwaltung verschiedener Aspekte des Systems:

1. **`generator.json`**: Konfiguration f√ºr Textgenerierungsmodelle
   - Definiert verf√ºgbare Model-Provider (Google, OpenAI, OpenRouter, Azure, Ollama)
   - Gibt Standard- und verf√ºgbare Modelle f√ºr jeden Provider an
   - Enth√§lt modellspezifische Parameter wie Temperatur und top_p

2. **`embedder.json`**: Konfiguration f√ºr Embedding-Modelle und Textverarbeitung
   - Definiert Embedding-Modelle f√ºr Vektorspeicherung
   - Enth√§lt Retriever-Konfiguration f√ºr RAG
   - Gibt Text-Splitter-Einstellungen f√ºr Dokumentenaufteilung an

3. **`repo.json`**: Konfiguration f√ºr Repository-Handling
   - Enth√§lt Dateifilter zur Ausschlie√üung bestimmter Dateien und Verzeichnisse
   - Definiert Repository-Gr√∂√üenlimits und Verarbeitungsregeln

Standardm√§√üig befinden sich diese Dateien im Verzeichnis `api/config/`. Sie k√∂nnen ihren Speicherort mithilfe der Umgebungsvariable `DEEPWIKI_CONFIG_DIR` anpassen.

### Benutzerdefinierte Modellauswahl f√ºr Service-Provider

Die Funktion der benutzerdefinierten Modellauswahl ist speziell f√ºr Service-Provider gedacht, die:

- Mehrere KI-Modelloptionen Benutzern in Ihrer Organisation anbieten k√∂nnen
- Sich schnell an die sich schnell entwickelnde LLM-Landschaft anpassen k√∂nnen, ohne Code zu √§ndern
- Spezialisierte oder feinabgestimmte Modelle unterst√ºtzen k√∂nnen, die nicht in der vordefinierten Liste enthalten sind

Service-Provider k√∂nnen ihre Modelle implementieren, indem sie aus den vordefinierten Optionen ausw√§hlen oder benutzerdefinierte Modellbezeichner in der Frontend-Schnittstelle eingeben.

### Base URL Konfiguration f√ºr Enterprise Private Channels

Die base_url-Konfiguration des OpenAI-Clients ist haupts√§chlich f√ºr Enterprise-Benutzer mit privaten API-Kan√§len konzipiert. Diese Funktion:

- Erm√∂glicht die Verbindung zu privaten oder unternehmensweiten API-Endpoints
- Erm√∂glicht Organisationen die Verwendung ihrer eigenen selbst gehosteten oder benutzerdefinierten LLM-Dienste
- Unterst√ºtzt die Integration mit OpenAI API-kompatiblen Diensten von Drittanbietern

**Demn√§chst**: In zuk√ºnftigen Updates wird DeepWiki einen Modus unterst√ºtzen, in dem Benutzer ihre eigenen API-Schl√ºssel in Anfragen bereitstellen m√ºssen. Dies erm√∂glicht Enterprise-Kunden mit privaten Kan√§len, ihre bestehenden API-Vereinbarungen zu nutzen, ohne Anmeldedaten mit der DeepWiki-Bereitstellung zu teilen.

## üß© Verwendung von OpenAI-kompatiblen Embedding-Modellen (z. B. Alibaba Qwen)

Wenn Sie Embedding-Modelle verwenden m√∂chten, die mit der OpenAI API kompatibel sind (wie Alibaba Qwen), f√ºhren Sie diese Schritte aus:

1. Ersetzen Sie den Inhalt von `api/config/embedder.json` durch den aus `api/config/embedder_openai_compatible.json`.
2. Legen Sie in Ihrer Projekt-Root `.env`-Datei die relevanten Umgebungsvariablen fest, z. B.:
   ```
   OPENAI_API_KEY=your_api_key
   OPENAI_BASE_URL=your_openai_compatible_endpoint
   ```
3. Das Programm ersetzt Platzhalter in der Datei embedder.json automatisch durch die Werte aus Ihren Umgebungsvariablen.

Dies erm√∂glicht es Ihnen, zu jedem OpenAI-kompatiblen Embedding-Service nahtlos zu wechseln, ohne Code√§nderungen vorzunehmen.

## üß† Verwendung von Google AI Embeddings

DeepWiki unterst√ºtzt nun die neuesten Embedding-Modelle von Google AI als Alternative zu OpenAI-Embeddings. Dies bietet eine bessere Integration, wenn Sie bereits Google Gemini-Modelle f√ºr die Textgenerierung verwenden.

### Funktionen

- **Neuestes Modell**: Verwendet Googles `text-embedding-004` Modell
- **Gleicher API-Schl√ºssel**: Verwendet Ihren vorhandenen `GOOGLE_API_KEY` (keine zus√§tzliche Einrichtung erforderlich)
- **Bessere Integration**: Optimiert f√ºr die Verwendung mit Google Gemini-Textgenerierungsmodellen
- **Aufgabenspezifisch**: Unterst√ºtzt semantische √Ñhnlichkeit, Abfrage und Klassifizierungsaufgaben
- **Batch-Verarbeitung**: Effiziente Verarbeitung mehrerer Texte

### So aktivieren Sie Google AI Embeddings

**Option 1: Umgebungsvariable (Empfohlen)**

Legen Sie den Embedder-Typ in Ihrer `.env`-Datei fest:

```bash
# Ihr vorhandener Google API-Schl√ºssel
GOOGLE_API_KEY=your_google_api_key

# Aktivieren Sie Google AI Embeddings
DEEPWIKI_EMBEDDER_TYPE=google
```

**Option 2: Docker-Umgebung**

```bash
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e DEEPWIKI_EMBEDDER_TYPE=google \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

**Option 3: Docker Compose**

F√ºgen Sie zu Ihrer `.env`-Datei hinzu:

```bash
GOOGLE_API_KEY=your_google_api_key
DEEPWIKI_EMBEDDER_TYPE=google
```

Dann f√ºhren Sie aus:

```bash
docker-compose up
```

### Verf√ºgbare Embedder-Typen

| Typ | Beschreibung | API-Schl√ºssel erforderlich | Notizen |
|------|-------------|------------------|-------|
| `openai` | OpenAI Embeddings (Standard) | `OPENAI_API_KEY` | Verwendet das `text-embedding-3-small` Modell |
| `google` | Google AI Embeddings | `GOOGLE_API_KEY` | Verwendet das `text-embedding-004` Modell |
| `ollama` | Lokale Ollama Embeddings | Keine | Erfordert lokale Ollama-Installation |

### Warum Google AI Embeddings verwenden?

- **Konsistenz**: Wenn Sie Google Gemini f√ºr die Textgenerierung verwenden, bieten Google-Embeddings bessere semantische Konsistenz
- **Leistung**: Googles neuestes Embedding-Modell bietet ausgezeichnete Leistung f√ºr Abrufaufgaben
- **Kosten**: Wettbewerbsf√§hige Preise im Vergleich zu OpenAI
- **Keine zus√§tzliche Einrichtung**: Verwendet denselben API-Schl√ºssel wie Ihre Textgenerierungsmodelle

### Wechsel zwischen Embeddern

Sie k√∂nnen problemlos zwischen verschiedenen Embedding-Anbietern wechseln:

```bash
# Verwenden Sie OpenAI Embeddings (Standard)
export DEEPWIKI_EMBEDDER_TYPE=openai

# Verwenden Sie Google AI Embeddings
export DEEPWIKI_EMBEDDER_TYPE=google

# Verwenden Sie lokale Ollama Embeddings
export DEEPWIKI_EMBEDDER_TYPE=ollama
```

**Hinweis**: Wenn Sie Embedder wechseln, m√ºssen Sie m√∂glicherweise Ihre Repository-Embeddings neu generieren, da verschiedene Modelle unterschiedliche Vektorr√§ume erzeugen.

### Protokollierung

DeepWiki verwendet Pythons eingebautes `logging`-Modul f√ºr Diagnoseausgaben. Sie k√∂nnen die Ausf√ºhrlichkeit und den Speicherort der Protokolldatei √ºber Umgebungsvariablen konfigurieren:

| Variable        | Beschreibung                                                        | Standard                      |
|-----------------|--------------------------------------------------------------------|------------------------------|
| `LOG_LEVEL`     | Protokollierungsstufe (DEBUG, INFO, WARNING, ERROR, CRITICAL).      | INFO                         |
| `LOG_FILE_PATH` | Pfad zur Protokolldatei. Falls gesetzt, werden Protokolle in diese Datei geschrieben. | `api/logs/application.log`   |

Zum Aktivieren von Debug-Protokollierung und zum Weiterleiten von Protokollen zu einer benutzerdefinierten Datei:
```bash
export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
```
Oder mit Docker Compose:
```bash
LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
```

Bei Ausf√ºhrung mit Docker Compose wird das Verzeichnis `api/logs` des Containers an `./api/logs` auf Ihrem Host gebunden (siehe Abschnitt `volumes` in `docker-compose.yml`), was die Persistenz von Protokolldateien √ºber Neustarts hinweg gew√§hrleistet.

Alternativ k√∂nnen Sie diese Einstellungen in Ihrer `.env`-Datei speichern:

```bash
LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
```
F√ºhren Sie dann einfach aus:

```bash
docker-compose up
```

**√úberlegungen zur Sicherheit des Protokollierungspfads:** Stellen Sie in Produktionsumgebungen sicher, dass das Verzeichnis `api/logs` und alle benutzerdefinierten Protokolldateipfade mit angemessenen Dateisystem-Berechtigungen und Zugriffskontrolle gesichert sind. Die Anwendung erzwingt, dass `LOG_FILE_PATH` im Verzeichnis `api/logs` des Projekts verbleibt, um Pfaddurchquerung oder nicht autorisierte Schreibvorg√§nge zu verhindern.

## üõ†Ô∏è Erweiterte Einrichtung

### Umgebungsvariablen

| Variable             | Beschreibung                                                  | Erforderlich | Notiz                                                                                                     |
|----------------------|--------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------|
| `GOOGLE_API_KEY`     | Google Gemini API-Schl√ºssel f√ºr KI-Generierung und Embeddings      | Nein | Erforderlich f√ºr Google Gemini-Modelle und Google AI-Embeddings                                               
| `OPENAI_API_KEY`     | OpenAI API-Schl√ºssel f√ºr Embeddings und Modelle                     | Bedingt | Erforderlich bei Verwendung von OpenAI-Embeddings oder -Modellen                                                            |
| `OPENROUTER_API_KEY` | OpenRouter API-Schl√ºssel f√ºr alternative Modelle                    | Nein | Nur erforderlich, wenn Sie OpenRouter-Modelle verwenden m√∂chten                                                       |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API-Schl√ºssel                    | Nein | Nur erforderlich, wenn Sie Azure OpenAI-Modelle verwenden m√∂chten                                                       |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI Endpoint                    | Nein | Nur erforderlich, wenn Sie Azure OpenAI-Modelle verwenden m√∂chten                                                       |
| `AZURE_OPENAI_VERSION` | Azure OpenAI Version                     | Nein | Nur erforderlich, wenn Sie Azure OpenAI-Modelle verwenden m√∂chten                                                       |
| `OLLAMA_HOST`        | Ollama Host (Standard: http://localhost:11434)                | Nein | Nur erforderlich, wenn Sie einen externen Ollama-Server verwenden m√∂chten                                                  |
| `DEEPWIKI_EMBEDDER_TYPE` | Embedder-Typ: `openai`, `google` oder `ollama` (Standard: `openai`) | Nein | Steuert, welcher Embedding-Provider verwendet werden soll                                                              |
| `PORT`               | Port f√ºr den API-Server (Standard: 8001)                      | Nein | Wenn Sie API und Frontend auf derselben Maschine hosten, stellen Sie sicher, dass Sie den Port von `SERVER_BASE_URL` entsprechend √§ndern |
| `SERVER_BASE_URL`    | Basis-URL f√ºr den API-Server (Standard: http://localhost:8001) | Nein |
| `DEEPWIKI_AUTH_MODE` | Setzen Sie auf `true` oder `1`, um den Autorisierungsmodus zu aktivieren. | Nein | Standardm√§√üig `false`. Falls aktiviert, ist `DEEPWIKI_AUTH_CODE` erforderlich. |
| `DEEPWIKI_AUTH_CODE` | Der geheime Code, der f√ºr die Wiki-Generierung erforderlich ist, wenn `DEEPWIKI_AUTH_MODE` aktiviert ist. | Nein | Wird nur verwendet, wenn `DEEPWIKI_AUTH_MODE` `true` oder `1` ist. |

**API-Schl√ºssel-Anforderungen:**
- Bei Verwendung von `DEEPWIKI_EMBEDDER_TYPE=openai` (Standard): `OPENAI_API_KEY` ist erforderlich
- Bei Verwendung von `DEEPWIKI_EMBEDDER_TYPE=google`: `GOOGLE_API_KEY` ist erforderlich  
- Bei Verwendung von `DEEPWIKI_EMBEDDER_TYPE=ollama`: Kein API-Schl√ºssel erforderlich (lokale Verarbeitung)

Andere API-Schl√ºssel sind nur erforderlich, wenn Modelle der entsprechenden Provider konfiguriert und verwendet werden.

## Autorisierungsmodus

DeepWiki kann so konfiguriert werden, dass es im Autorisierungsmodus ausgef√ºhrt wird, bei dem die Wiki-Generierung einen g√ºltigen Autorisierungscode erfordert. Dies ist n√ºtzlich, wenn Sie steuern m√∂chten, wer die Generierungsfunktion verwenden kann.
Beschr√§nkt die Frontend-Initiierung und sch√ºtzt das L√∂schen des Caches, verhindert aber nicht vollst√§ndig die Backend-Generierung, wenn API-Endpoints direkt aufgerufen werden.

Um den Autorisierungsmodus zu aktivieren, setzen Sie die folgenden Umgebungsvariablen:

- `DEEPWIKI_AUTH_MODE`: Setzen Sie dies auf `true` oder `1`. Wenn aktiviert, zeigt das Frontend ein Eingabefeld f√ºr den Autorisierungscode an.
- `DEEPWIKI_AUTH_CODE`: Setzen Sie dies auf den gew√ºnschten geheimen Code. Beschr√§nkt die Frontend-Initiierung und sch√ºtzt das L√∂schen des Caches, verhindert aber nicht vollst√§ndig die Backend-Generierung, wenn API-Endpoints direkt aufgerufen werden.

Wenn `DEEPWIKI_AUTH_MODE` nicht gesetzt oder auf `false` (oder einen anderen Wert als `true`/`1`) gesetzt ist, wird die Autorisierungsfunktion deaktiviert, und kein Code ist erforderlich.

### Docker-Einrichtung

Sie k√∂nnen Docker verwenden, um DeepWiki auszuf√ºhren:

#### Container ausf√ºhren

```bash
# Rufen Sie das Image aus GitHub Container Registry ab
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# F√ºhren Sie den Container mit Umgebungsvariablen aus
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e OLLAMA_HOST=your_ollama_host \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \

  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

Dieser Befehl bindet auch `~/.adalflow` auf Ihrem Host an `/root/.adalflow` im Container. Dieser Pfad wird verwendet, um zu speichern:
- Geklonte Repositories (`~/.adalflow/repos/`)
- Ihre Embeddings und Indizes (`~/.adalflow/databases/`)
- Zwischengespeicherte generierte Wiki-Inhalte (`~/.adalflow/wikicache/`)

Dies stellt sicher, dass Ihre Daten bestehen bleiben, selbst wenn der Container gestoppt oder entfernt wird.

Oder verwenden Sie die bereitgestellte Datei `docker-compose.yml`:

```bash
# Bearbeiten Sie die .env-Datei zun√§chst mit Ihren API-Schl√ºsseln
docker-compose up
```

(Die Datei `docker-compose.yml` ist vorkonfiguriert, um `~/.adalflow` f√ºr Datenpersistenz bereitzustellen, √§hnlich dem obigen `docker run`-Befehl.)

#### Verwendung einer .env-Datei mit Docker

Sie k√∂nnen auch eine .env-Datei dem Container hinzuf√ºgen:

```bash
# Erstellen Sie eine .env-Datei mit Ihren API-Schl√ºsseln
echo "GOOGLE_API_KEY=your_google_api_key" > .env
echo "OPENAI_API_KEY=your_openai_api_key" >> .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" >> .env
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" >> .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" >> .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version"  >>.env
echo "OLLAMA_HOST=your_ollama_host" >> .env

# F√ºhren Sie den Container mit der gebundenen .env-Datei aus
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

Dieser Befehl bindet auch `~/.adalflow` auf Ihrem Host an `/root/.adalflow` im Container. Dieser Pfad wird verwendet, um zu speichern:
- Geklonte Repositories (`~/.adalflow/repos/`)
- Ihre Embeddings und Indizes (`~/.adalflow/databases/`)
- Zwischengespeicherte generierte Wiki-Inhalte (`~/.adalflow/wikicache/`)

Dies stellt sicher, dass Ihre Daten bestehen bleiben, selbst wenn der Container gestoppt oder entfernt wird.

#### Docker-Image lokal erstellen

Wenn Sie das Docker-Image lokal erstellen m√∂chten:

```bash
# Repository klonen
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Docker-Image erstellen
docker build -t deepwiki-open .

# Container ausf√ºhren
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \
  -e OLLAMA_HOST=your_ollama_host \
  deepwiki-open
```

#### Verwendung von selbstsignierte Zertifikaten in Docker

Wenn Sie sich in einer Umgebung befinden, die selbstsignierte Zertifikate verwendet, k√∂nnen Sie diese in den Docker-Build einbeziehen:

1. Erstellen Sie ein Verzeichnis f√ºr Ihre Zertifikate (Standard ist `certs` in Ihrem Projekt-Stammverzeichnis)
2. Kopieren Sie Ihre `.crt` oder `.pem` Zertifikatsdateien in dieses Verzeichnis
3. Erstellen Sie das Docker-Image:

```bash
# Erstellen Sie mit dem Standard-Zertifikateverzeichnis (certs)
docker build .

# Oder erstellen Sie mit einem benutzerdefinierten Zertifikateverzeichnis
docker build --build-arg CUSTOM_CERT_DIR=my-custom-certs .
```

### API-Server-Details

Der API-Server bietet:
- Repository-Klonen und Indizierung
- RAG (Retrieval Augmented Generation)
- Streaming-Chat-Vervollst√§ndigungen

Weitere Details finden Sie unter [API README](./api/README.md).

## üîå OpenRouter-Integration

DeepWiki unterst√ºtzt jetzt [OpenRouter](https://openrouter.ai/) als Model-Provider und bietet Ihnen Zugriff auf Hunderte von KI-Modellen durch eine einzelne API:

- **Mehrere Modelloptionen**: Zugriff auf Modelle von OpenAI, Anthropic, Google, Meta, Mistral und mehr
- **Einfache Konfiguration**: F√ºgen Sie einfach Ihren OpenRouter API-Schl√ºssel hinzu und w√§hlen Sie das gew√ºnschte Modell aus
- **Kosteneffizienz**: W√§hlen Sie Modelle, die zu Ihrem Budget und Leistungsbedarf passen
- **Einfaches Wechseln**: Wechseln Sie ohne Code√§nderungen zwischen verschiedenen Modellen

### Verwendung von OpenRouter mit DeepWiki

1. **Holen Sie sich einen API-Schl√ºssel**: Melden Sie sich bei [OpenRouter](https://openrouter.ai/) an und rufen Sie Ihren API-Schl√ºssel ab
2. **Hinzuf√ºgen zur Umgebung**: F√ºgen Sie `OPENROUTER_API_KEY=your_key` zu Ihrer `.env`-Datei hinzu
3. **In UI aktivieren**: Aktivieren Sie die Option "OpenRouter API verwenden" auf der Homepage
4. **W√§hlen Sie Modell**: W√§hlen Sie aus beliebten Modellen wie GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 und mehr

OpenRouter ist besonders n√ºtzlich, wenn Sie:
- Verschiedene Modelle ausprobieren m√∂chten, ohne sich f√ºr mehrere Services anzumelden
- Auf Modelle zugreifen m√∂chten, die in Ihrer Region m√∂glicherweise eingeschr√§nkt sind
- Leistungsvergleiche √ºber verschiedene Model-Provider hinweg durchf√ºhren m√∂chten
- Die Kosten vs. Leistung auf Grundlage Ihrer Bed√ºrfnisse optimieren m√∂chten

## ü§ñ Ask & DeepResearch-Funktionen

### Ask-Funktion

Die Ask-Funktion erm√∂glicht es Ihnen, mit Ihrem Repository mithilfe von Retrieval Augmented Generation (RAG) zu chatten:

- **Kontextbewusste Antworten**: Erhalten Sie genaue Antworten basierend auf dem tats√§chlichen Code in Ihrem Repository
- **RAG-Powered**: Das System ruft relevante Code-Snippets ab, um fundierte Antworten zu geben
- **Echtzeitstreaming**: Sehen Sie die Antworten w√§hrend ihrer Generierung f√ºr ein interaktiveres Erlebnis
- **Konversationsverlauf**: Das System beh√§lt den Kontext zwischen Fragen f√ºr koh√§rentere Wechselwirkungen bei

### DeepResearch-Funktion

DeepResearch f√ºhrt die Repository-Analyse auf die n√§chste Stufe mit einem mehrteiligen Rechercheprozess:

- **Tiefgr√ºndige Untersuchung**: Untersucht komplexe Themen gr√ºndlich durch mehrere Recherche-Iterationen
- **Strukturierter Prozess**: Folgt einem klaren Forschungsplan mit Updates und einer umfassenden Schlussfolgerung
- **Automatische Fortsetzung**: Die KI setzt die Forschung automatisch fort, bis eine Schlussfolgerung erreicht wird (bis zu 5 Iterationen)
- **Forschungsstufen**:
  1. **Forschungsplan**: Beschreibt den Ansatz und die ersten Erkenntnisse
  2. **Forschungsupdates**: Baut auf vorherigen Iterationen mit neuen Erkenntnissen auf
  3. **Abschlie√üende Schlussfolgerung**: Bietet eine umfassende Antwort basierend auf allen Iterationen

Um DeepResearch zu verwenden, aktivieren Sie einfach den Schieberegler "Tiefe Recherche" in der Ask-Schnittstelle, bevor Sie Ihre Frage einreichen.

## üì± Screenshots

![DeepWiki Hauptschnittstelle](screenshots/Interface.png)
*Die Hauptschnittstelle von DeepWiki*

![Unterst√ºtzung f√ºr private Repositories](screenshots/privaterepo.png)
*Zugriff auf private Repositories mit pers√∂nlichen Zugriffstokens*

![DeepResearch-Funktion](screenshots/DeepResearch.png)
*DeepResearch f√ºhrt mehrteilige Untersuchungen f√ºr komplexe Themen durch*

### Demo-Video

[![DeepWiki Demo-Video](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*Sehen Sie DeepWiki in Aktion!*

## ‚ùì Fehlerbehebung

### API-Schl√ºssel-Probleme
- **"Fehlende Umgebungsvariablen"**: Stellen Sie sicher, dass sich Ihre `.env`-Datei im Projekt-Stammverzeichnis befindet und die erforderlichen API-Schl√ºssel enth√§lt
- **"API-Schl√ºssel nicht g√ºltig"**: √úberpr√ºfen Sie, ob Sie den vollst√§ndigen Schl√ºssel korrekt kopiert haben, ohne zus√§tzliche Leerzeichen
- **"OpenRouter API-Fehler"**: √úberpr√ºfen Sie, ob Ihr OpenRouter API-Schl√ºssel g√ºltig ist und ausreichend Guthaben verf√ºgbar ist
- **"Azure OpenAI API-Fehler"**: √úberpr√ºfen Sie, ob Ihre Azure OpenAI-Anmeldedaten (API-Schl√ºssel, Endpoint und Version) korrekt sind und der Dienst ordnungsgem√§√ü bereitgestellt ist

### Verbindungsprobleme
- **"Kann keine Verbindung zum API-Server herstellen"**: Stellen Sie sicher, dass der API-Server auf Port 8001 ausgef√ºhrt wird
- **"CORS-Fehler"**: Die API ist so konfiguriert, dass alle Urspr√ºnge zul√§ssig sind, aber wenn Sie Probleme haben, versuchen Sie, sowohl Frontend als auch Backend auf derselben Maschine auszuf√ºhren

### Generierungsprobleme
- **"Fehler beim Generieren des Wikis"**: Versuchen Sie zun√§chst ein kleineres Repository f√ºr sehr gro√üe Repositories
- **"Ung√ºltiges Repository-Format"**: Stellen Sie sicher, dass Sie ein g√ºltiges GitHub-, GitLab- oder Bitbucket-URL-Format verwenden
- **"Konnte Repository-Struktur nicht abrufen"**: Stellen Sie f√ºr private Repositories sicher, dass Sie einen g√ºltigen pers√∂nlichen Zugrifftoken mit angemessenen Berechtigungen eingegeben haben
- **"Diagramm-Rendering-Fehler"**: Die App versucht automatisch, fehlerhafte Diagramme zu reparieren

### H√§ufige L√∂sungen
1. **Starten Sie beide Server neu**: Manchmal behebt ein einfacher Neustart die meisten Probleme
2. **√úberpr√ºfen Sie die Konsolenprotokolle**: √ñffnen Sie die Browser-Entwicklertools, um JavaScript-Fehler anzuzeigen
3. **√úberpr√ºfen Sie API-Protokolle**: Sehen Sie sich das Terminal an, in dem die API ausgef√ºhrt wird, um Python-Fehler zu sehen

## ü§ù Beitragen

Beitr√§ge sind willkommen! Sie k√∂nnen gerne:
- Probleme f√ºr Fehler oder Funktionsw√ºnsche er√∂ffnen
- Pull-Anfragen einreichen, um den Code zu verbessern
- Ihr Feedback und Ihre Ideen teilen

## üìÑ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert - siehe die [LICENSE](LICENSE)-Datei f√ºr Details.

## ‚≠ê Star-Verlauf

[![Star History Chart](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)

